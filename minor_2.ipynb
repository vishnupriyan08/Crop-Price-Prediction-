{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"16baojY9IRrkwR2rKhfhkMUb9ProN7oRF","authorship_tag":"ABX9TyN11+kTJGuZeC8bu/IyfS4U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkXUyv0oZb9t","executionInfo":{"status":"ok","timestamp":1714582486048,"user_tz":-330,"elapsed":1917958,"user":{"displayName":"VISHNUPRIYAN C,AI & MACHINE LEARNING2021 Vel Tech, Chennai","userId":"00786642757298524686"}},"outputId":"79389823-4941-4370-fbc9-6b633b1f6cff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train - PA-cutaneous-larva-migrans: 100 images\n","Train - FU-ringworm: 90 images\n","Train - VI-chickenpox: 136 images\n","Train - BA-impetigo: 90 images\n","Train - BA- cellulitis: 136 images\n","Train - FU-athlete-foot: 124 images\n","Train - VI-shingles: 130 images\n","Train - FU-nail-fungus: 129 images\n","Test - VI-shingles: 49 images\n","Test - VI-chickenpox: 34 images\n","Test - BA- cellulitis: 34 images\n","Test - BA-impetigo: 20 images\n","Test - FU-nail-fungus: 33 images\n","Test - FU-ringworm: 23 images\n","Test - PA-cutaneous-larva-migrans: 25 images\n","Test - FU-athlete-foot: 32 images\n","Total train images: 935\n","Total test images: 250\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_14 (Conv2D)          (None, 150, 150, 32)      896       \n","                                                                 \n"," max_pooling2d_14 (MaxPooli  (None, 75, 75, 32)        0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_15 (Conv2D)          (None, 75, 75, 64)        18496     \n","                                                                 \n"," max_pooling2d_15 (MaxPooli  (None, 37, 37, 64)        0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_16 (Conv2D)          (None, 37, 37, 128)       73856     \n","                                                                 \n"," max_pooling2d_16 (MaxPooli  (None, 18, 18, 128)       0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_17 (Conv2D)          (None, 18, 18, 128)       147584    \n","                                                                 \n"," max_pooling2d_17 (MaxPooli  (None, 9, 9, 128)         0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_18 (Conv2D)          (None, 9, 9, 128)         147584    \n","                                                                 \n"," max_pooling2d_18 (MaxPooli  (None, 4, 4, 128)         0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_19 (Conv2D)          (None, 4, 4, 512)         590336    \n","                                                                 \n"," max_pooling2d_19 (MaxPooli  (None, 2, 2, 512)         0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_20 (Conv2D)          (None, 2, 2, 512)         2359808   \n","                                                                 \n"," max_pooling2d_20 (MaxPooli  (None, 1, 1, 512)         0         \n"," ng2D)                                                           \n","                                                                 \n"," flatten_2 (Flatten)         (None, 512)               0         \n","                                                                 \n"," dense_6 (Dense)             (None, 512)               262656    \n","                                                                 \n"," dense_7 (Dense)             (None, 128)               65664     \n","                                                                 \n"," dense_8 (Dense)             (None, 8)                 1032      \n","                                                                 \n","=================================================================\n","Total params: 3667912 (13.99 MB)\n","Trainable params: 3667912 (13.99 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Found 934 images belonging to 8 classes.\n","Found 249 images belonging to 8 classes.\n","Epoch 1/25\n","47/47 [==============================] - 71s 1s/step - loss: 2.0754 - accuracy: 0.1263\n","Epoch 2/25\n","47/47 [==============================] - 70s 1s/step - loss: 2.0720 - accuracy: 0.1349\n","Epoch 3/25\n","47/47 [==============================] - 69s 1s/step - loss: 2.0282 - accuracy: 0.2024\n","Epoch 4/25\n","47/47 [==============================] - 69s 1s/step - loss: 1.9386 - accuracy: 0.2323\n","Epoch 5/25\n","47/47 [==============================] - 68s 1s/step - loss: 1.8695 - accuracy: 0.2570\n","Epoch 6/25\n","47/47 [==============================] - 68s 1s/step - loss: 1.7380 - accuracy: 0.3287\n","Epoch 7/25\n","47/47 [==============================] - 68s 1s/step - loss: 1.6040 - accuracy: 0.3929\n","Epoch 8/25\n","47/47 [==============================] - 70s 1s/step - loss: 1.3991 - accuracy: 0.4647\n","Epoch 9/25\n","47/47 [==============================] - 68s 1s/step - loss: 1.2040 - accuracy: 0.5321\n","Epoch 10/25\n","47/47 [==============================] - 70s 1s/step - loss: 1.0616 - accuracy: 0.6231\n","Epoch 11/25\n","47/47 [==============================] - 68s 1s/step - loss: 0.8170 - accuracy: 0.6981\n","Epoch 12/25\n","47/47 [==============================] - 68s 1s/step - loss: 0.7179 - accuracy: 0.7463\n","Epoch 13/25\n","47/47 [==============================] - 70s 1s/step - loss: 0.5734 - accuracy: 0.8030\n","Epoch 14/25\n","47/47 [==============================] - 68s 1s/step - loss: 0.4316 - accuracy: 0.8437\n","Epoch 15/25\n","47/47 [==============================] - 68s 1s/step - loss: 0.3455 - accuracy: 0.8715\n","Epoch 16/25\n","47/47 [==============================] - 70s 1s/step - loss: 0.3512 - accuracy: 0.8704\n","Epoch 17/25\n","47/47 [==============================] - 69s 1s/step - loss: 0.1992 - accuracy: 0.9379\n","Epoch 18/25\n","47/47 [==============================] - 68s 1s/step - loss: 0.2090 - accuracy: 0.9379\n","Epoch 19/25\n","47/47 [==============================] - 67s 1s/step - loss: 0.2731 - accuracy: 0.9058\n","Epoch 20/25\n","47/47 [==============================] - 68s 1s/step - loss: 0.2196 - accuracy: 0.9325\n","Epoch 21/25\n","47/47 [==============================] - 68s 1s/step - loss: 0.1278 - accuracy: 0.9561\n","Epoch 22/25\n","47/47 [==============================] - 66s 1s/step - loss: 0.1987 - accuracy: 0.9390\n","Epoch 23/25\n","47/47 [==============================] - 68s 1s/step - loss: 0.0949 - accuracy: 0.9775\n","Epoch 24/25\n","47/47 [==============================] - 68s 1s/step - loss: 0.0552 - accuracy: 0.9839\n","Epoch 25/25\n","47/47 [==============================] - 68s 1s/step - loss: 0.0103 - accuracy: 0.9989\n","13/13 [==============================] - 6s 436ms/step - loss: 1.2968 - accuracy: 0.8233\n","the test accuracy :  0.823293149471283\n"]}],"source":["\n","# Importing necessary libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","from PIL import Image\n","from tensorflow.keras import models, layers, optimizers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import tensorflow as tf\n","# Data handling\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import cv2\n","from PIL import Image\n","from tensorflow.keras import models, layers, optimizers, losses, metrics\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import tensorflow as tf\n","import os\n","from pathlib import Path\n","import random\n","# Define the path to your dataset directory in Google Drive\n","data_dir = '/content/drive/MyDrive/Minor_project_2/skin-disease-datasaet'\n","\n","\n","# Specify the parent directory containing train and test subdirectories\n","train_dir = os.path.join(data_dir, 'train_set')\n","test_dir = os.path.join(data_dir, 'test_set')\n","\n","# Initialize counters\n","total_train_images = 0\n","total_test_images = 0\n","\n","# Count images in train set\n","for category in os.listdir(train_dir):\n","    category_dir = os.path.join(train_dir, category)\n","    num_images = len(os.listdir(category_dir))\n","    print(f\"Train - {category}: {num_images} images\")\n","    total_train_images += num_images\n","\n","# Count images in test set\n","for category in os.listdir(test_dir):\n","    category_dir = os.path.join(test_dir, category)\n","    num_images = len(os.listdir(category_dir))\n","    print(f\"Test - {category}: {num_images} images\")\n","    total_test_images += num_images\n","\n","# Calculate total number of images\n","print(f\"Total train images: {total_train_images}\")\n","print(f\"Total test images: {total_test_images}\")\n","\n","\n","# Building the model architecture\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(150, 150, 3)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Flatten())\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(128, activation='relu'))\n","model.add(layers.Dense(8, activation='softmax'))\n","\n","model.summary()\n","\n","# Data preprocessing\n","\n","# List all class subdirectories\n","class_subdirs = os.listdir(data_dir)\n","\n","# Initialize empty lists for images and labels\n","train_images = []\n","train_labels = []\n","\n","# Use ImageDataGenerator to load and preprocess images\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(150, 150),\n","    batch_size=20,\n","    class_mode='categorical'\n",")\n","\n","test_generator = test_datagen.flow_from_directory(\n","    test_dir,\n","    target_size=(150, 150),\n","    batch_size=20,\n","    class_mode='categorical'\n",")\n","\n","# Compile the model\n","model.compile(\n","    optimizer='adam',\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","# Train the model\n","history = model.fit(\n","    train_generator,\n","    epochs=25,\n","    batch_size=15,\n",")\n","\n","# Evaluate the model\n","test_loss, test_acc = model.evaluate(test_generator)\n","\n","# Save the model weights\n","model.save_weights(\"/content/drive/MyDrive/Minor_project_2/my_model_weights.h5\")\n","\n","print(\"the test accuracy : \",test_acc)"]}]}